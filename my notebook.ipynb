{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb608d02-8fb3-422c-b076-6e7834eae9fe",
   "metadata": {},
   "source": [
    "# Hiding Information in AI Explanation _ Project Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c68f6a-0120-40f7-93fc-cd07e8aceb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to run the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe67f8-d079-4342-ac06-aaa0ab2ecd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/pankessel/adv_explanation_ref.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be969636-9863-43e7-b9e4-ebbad6ad7191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: numpy in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 1)) (1.24.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 2)) (3.8.0)\n",
      "Requirement already satisfied: torchcam in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 6)) (2.7.1+cu126)\n",
      "Requirement already satisfied: torchvision in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 7)) (0.22.1+cu126)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from -r requirements.txt (line 8)) (2.7.1+cu126)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (2023.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\julien\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 6)) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722cd666-6612-4d84-8ae2-ca799a815292",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:/users/julien/rai_project_LOCAL/adv_explanation_ref/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c94509d-22e0-4ee9-9e28-07a6202e5a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julien\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no display found. Using non-interactive Agg backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julien\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Julien\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchcam.methods import GradCAM\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path().resolve() / \"src\"))\n",
    "\n",
    "from nn.utils import get_expl, load_image\n",
    "from nn.enums import ExplainingMethod\n",
    "from nn.networks import ExplainableNet\n",
    "import torchvision.models as models\n",
    "\n",
    "imagenet_classes = requests.get(\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\").text.splitlines()\n",
    "\n",
    "model = models.resnet34(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "model_gradcam = models.resnet34(pretrained=True)\n",
    "model_gradcam.eval()\n",
    "cam_extractor = GradCAM(model_gradcam, target_layer=model_gradcam.layer4[-1])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_image(input_image, output_path):\n",
    "    \n",
    "    image_rgb = input_image.convert(\"RGB\")\n",
    "    image_rgb.save(output_path, format=\"PNG\")\n",
    "    \n",
    "    return Image.open(output_path)\n",
    "\n",
    "\n",
    "    \n",
    "def classify(image):\n",
    "    \n",
    "    input_tensor = transform(image).unsqueeze(0)  # [1, 3, 224, 224]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        top_prob, top_class = probs.topk(1, dim=1)\n",
    "\n",
    "    class_name = imagenet_classes[top_class.item()]\n",
    "    probability = top_prob.item()\n",
    "    return class_name, probability\n",
    "\n",
    "\n",
    "def image_from_text(text, width, height):\n",
    "    image_size = width, height\n",
    "    font_path = \"C:/Windows/Fonts/georgia.ttf\"\n",
    "    max_font_size = 500\n",
    "\n",
    "    def find_best_font_size(text, image_size, font_path, max_size=500):\n",
    "        for font_size in reversed(range(10, max_size, 2)):\n",
    "            try:\n",
    "                font = ImageFont.truetype(font_path, font_size)\n",
    "            except:\n",
    "                continue\n",
    "            dummy_img = Image.new('L', image_size)\n",
    "            draw = ImageDraw.Draw(dummy_img)\n",
    "            bbox = draw.textbbox((0, 0), text, font=font)\n",
    "            width, height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "            if width <= image_size[0] and height <= image_size[1]:\n",
    "                return font_size\n",
    "        return 10\n",
    "\n",
    "    font_size = find_best_font_size(text, image_size, font_path, max_font_size)\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "\n",
    "    img = Image.new('L', image_size, color=0)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    bbox = draw.textbbox((0, 0), text, font=font)\n",
    "    text_width, text_height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "    text_position = ((image_size[0] - text_width) // 2, (image_size[1] - text_height) // 2)\n",
    "    draw.text(text_position, text, fill=255, font=font)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "#gradcam method\n",
    "def saliency_map_gen(input_image, model=model_gradcam, device=\"cuda\"):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    if isinstance(input_image, Image.Image):\n",
    "        x = preprocess(input_image).unsqueeze(0).to(device)\n",
    "    elif isinstance(input_image, torch.Tensor):\n",
    "        x = input_image.clone().detach().to(device)\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported input type for saliency_map_gen\")\n",
    "\n",
    "    x.requires_grad = True\n",
    "\n",
    "    output = model(x)\n",
    "    class_idx = output.argmax(dim=1).item()\n",
    "    score = output[0, class_idx]\n",
    "    score.backward()\n",
    "\n",
    "    saliency = x.grad * x\n",
    "    saliency = saliency.abs().sum(dim=1, keepdim=True)\n",
    "    saliency = saliency - saliency.min()\n",
    "    saliency = saliency / (saliency.sum() + 1e-8)\n",
    "    saliency = F.interpolate(saliency, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "    return saliency.detach()\n",
    "\n",
    "\n",
    "\n",
    "#lrp method\n",
    "def get_lrp_map(image_path, model=None, device=None, folder_path=None):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    #ImageNet normalisation\n",
    "    data_mean = [0.485, 0.456, 0.406]\n",
    "    data_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    full_path = Path(os.path.join(folder_path, image_path)) if folder_path else Path(image_path)\n",
    "    x = load_image(data_mean, data_std, device, full_path)\n",
    "\n",
    "    if model is None:\n",
    "        base_model = models.vgg16(pretrained=True)\n",
    "        model = ExplainableNet(model=base_model).eval().to(device)\n",
    "\n",
    "    expl_map, _, _ = get_expl(model, x, ExplainingMethod.lrp)\n",
    "\n",
    "    return expl_map.squeeze(0).detach()\n",
    "\n",
    "\n",
    "\n",
    "def run(input_image, text_to_encode):\n",
    "    \n",
    "    #cleaning the temporary files\n",
    "    temp_folder_path = os.path.join(folder_path, \"temp/\")\n",
    "    \n",
    "    if os.path.exists(temp_folder_path):\n",
    "        shutil.rmtree(temp_folder_path)\n",
    "        \n",
    "    os.makedirs(temp_folder_path)\n",
    "\n",
    "\n",
    "    #taking care of the original input image\n",
    "    temp_path_input_image = os.path.join(folder_path, \"temp/temp_input_image.png\")\n",
    "    preprocessed_input_image = preprocess_image(input_image, temp_path_input_image)\n",
    "\n",
    "    \n",
    "    #taking care of the text\n",
    "    width,height = input_image.size\n",
    "    text_image = image_from_text(text_to_encode, width, height)\n",
    "    temp_path_text_image = os.path.join(folder_path, \"temp/temp_text_image.png\")\n",
    "    preprocessed_text_image = preprocess_image(text_image, temp_path_text_image)\n",
    "\n",
    "    #checking the original classification and its probability\n",
    "    print(\"checking the original class and probability...\")\n",
    "    original_class, original_probability = classify(preprocessed_input_image)\n",
    "\n",
    "    python_file_folder_path = os.path.join(folder_path, \"src/\")\n",
    "    print(\"running the script...\")\n",
    "    subprocess.run([\"python\", \"run_attack.py\", \"--cuda\", \"--img\", temp_path_input_image, \"--target_img\", temp_path_text_image], cwd=python_file_folder_path)\n",
    "\n",
    "    \n",
    "    outputs_path = os.path.join(folder_path, \"output/\")\n",
    "\n",
    "    print(\"copying the images...\")\n",
    "    images = {}\n",
    "    for i in [\"manipulated_image.png\", \"original_expl.png\", \"target_expl.png\", \"manipulated_expl.png\"]:\n",
    "        shutil.copy(os.path.join(folder_path, f\"output/{i}\"), os.path.join(folder_path, f\"temp/temp_{i}\"))\n",
    "        key = i.replace(\".png\", \"\")\n",
    "        images[key] = Image.open(os.path.join(folder_path, f\"temp/temp_{i}\"))\n",
    "  \n",
    "    original_expl = images[\"original_expl\"]\n",
    "    target_expl = images[\"target_expl\"]\n",
    "    manipulated_expl = images[\"manipulated_expl\"]\n",
    "    manipulated_image = images[\"manipulated_image\"]\n",
    " \n",
    "  \n",
    "    new_class, new_probability = classify(manipulated_image)\n",
    "\n",
    "    return original_class, original_probability, original_expl, target_expl, manipulated_expl, manipulated_image, new_class, new_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb98a59-a1b5-49fb-9192-c9c1f2117b76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo = gr.Interface(fn=run,\n",
    "                       inputs=[gr.Image(type=\"pil\"), \"text\"],\n",
    "                       outputs=[\"text\", \"text\", gr.Image(type=\"pil\", label=\"Original saliency\"), gr.Image(type=\"pil\", label=\"Text saliency\"), gr.Image(type=\"pil\", label=\"Manipulated saliency\"), gr.Image(type=\"pil\", label=\"Reconstructed image\"), \"text\", \"text\"],\n",
    "                       title=\"Hiding information in AI explanations\",\n",
    "                       description=\"Here you can upload a picture of your choice and choose the text that should be displayed as a \")\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c512a77-4d72-4609-b374-8b87b54faa94",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#manipulated image explanation can be tested here\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp/temp_manipulated_image.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m lrp_map \u001b[38;5;241m=\u001b[39m get_lrp_map(image_path, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(lrp_map\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#manipulated image explanation can be tested here\n",
    "\n",
    "image_path = os.path.join(folder_path, \"temp/temp_manipulated_image.png\")\n",
    "\n",
    "lrp_map = get_lrp_map(image_path, device=\"cuda\")\n",
    "plt.imshow(lrp_map.cpu().numpy(), cmap=\"hot\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af50b47c-4c6c-48a5-bf56-a60acc7af897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python C:\\users\\julien\\rai_project_LOCAL\\adv_explanation_ref\\src\\run_attack.py --cuda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
